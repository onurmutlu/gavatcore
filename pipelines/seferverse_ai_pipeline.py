from infrastructure.config.logger import get_logger

#!/usr/bin/env python3
"""
SEFERVERSE AI PIPELINE v2.0 - GPT-4 Powered
===========================================

Otomatik AI content generation ve scheduling sistemi.
GÃ¼nde 12 post, 4 kategori, akÄ±llÄ± zamanlama.

Ã–zellikler:
- GPT-4 ile quality content
- Smart scheduling (peak hours)
- Analytics tracking  
- Multi-platform sharing
- Auto hashtag generation
- Engagement optimization
"""

import asyncio
import json
import os
import schedule
import time
from datetime import datetime, timedelta
from typing import Dict, List
import structlog

# AI Generator import
from core.ai_content_generator import ai_content_generator

logger = structlog.get_logger("seferverse_ai_pipeline")

class SeferVerseAIPipeline:
    """SeferVerse AI Pipeline - AkÄ±llÄ± iÃ§erik Ã¼retim sistemi"""
    
    def __init__(self):
        self.is_running = False
        self.daily_posts_target = 12
        self.content_distribution = {
            "motivation": 4,  # En popÃ¼ler
            "lifestyle": 3,
            "tech": 3, 
            "business": 2
        }
        
        # Peak hours for posting (Turkish time)
        self.peak_hours = [9, 12, 15, 18, 20, 22]
        
        # Storage paths
        self.generated_content_dir = "data/seferverse_content"
        self.analytics_dir = "data/analytics"
        
        # Ensure directories exist
        os.makedirs(self.generated_content_dir, exist_ok=True)
        os.makedirs(self.analytics_dir, exist_ok=True)
        
        self.daily_stats = {
            "posts_generated": 0,
            "posts_published": 0,
            "engagement_total": 0,
            "categories": {},
            "start_time": None
        }
    
    async def initialize(self):
        """Pipeline'Ä± baÅŸlat"""
        try:
            logger.info("ğŸš€ SeferVerse AI Pipeline baÅŸlatÄ±lÄ±yor...")
            
            # AI Content Generator'Ä± baÅŸlat
            await ai_content_generator.initialize()
            
            # Schedule jobs
            self._setup_scheduling()
            
            self.is_running = True
            logger.info("âœ… SeferVerse AI Pipeline hazÄ±r!")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Pipeline baÅŸlatma hatasÄ±: {e}")
            return False
    
    def _setup_scheduling(self):
        """Daily scheduling setup"""
        # Peak hours'da post scheduling
        for hour in self.peak_hours:
            schedule.every().day.at(f"{hour:02d}:00").do(self._scheduled_post_generation)
        
        # Daily analytics
        schedule.every().day.at("23:30").do(self._daily_analytics_report)
        
        # Weekly batch generation
        schedule.every().sunday.at("00:00").do(self._weekly_content_batch)
        
        logger.info(f"ğŸ“… Zamanlama kuruldu: {len(self.peak_hours)} daily slots")
    
    async def generate_daily_content(self) -> Dict:
        """GÃ¼nlÃ¼k iÃ§erik package'Ä± Ã¼ret"""
        try:
            start_time = datetime.now()
            self.daily_stats["start_time"] = start_time.isoformat()
            
            logger.info(f"ğŸ¯ GÃ¼nlÃ¼k iÃ§erik Ã¼retimi baÅŸlÄ±yor: {self.daily_posts_target} post")
            
            # Batch content generation
            posts = await ai_content_generator.generate_batch_content(
                count=self.daily_posts_target,
                distribution=self.content_distribution
            )
            
            if not posts:
                logger.error("âŒ Ä°Ã§erik Ã¼retilemedi!")
                return None
            
            # Timing optimization
            scheduled_posts = self._optimize_posting_schedule(posts)
            
            # Save to files
            daily_package = {
                "date": start_time.strftime("%Y-%m-%d"),
                "total_posts": len(posts),
                "categories": self._analyze_categories(posts),
                "scheduled_posts": scheduled_posts,
                "generated_at": start_time.isoformat(),
                "estimated_engagement": self._calculate_total_engagement(posts),
                "posts": posts
            }
            
            # Save daily package
            filename = f"{self.generated_content_dir}/daily_{start_time.strftime('%Y%m%d')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(daily_package, f, ensure_ascii=False, indent=2)
            
            # Update stats
            self.daily_stats["posts_generated"] = len(posts)
            self.daily_stats["categories"] = self._analyze_categories(posts)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"""
âœ… GÃœNLÃœK Ä°Ã‡ERÄ°K PACKAGE HAZIR!

ğŸ“Š Ä°statistikler:
   ğŸ¯ Ãœretilen: {len(posts)} post
   â±ï¸ SÃ¼re: {duration:.1f} saniye
   ğŸ“ˆ Engagement tahmini: {daily_package['estimated_engagement']:.1f}%
   ğŸ“ Dosya: {filename}

ğŸ“… Kategori DaÄŸÄ±lÄ±mÄ±:
{self._format_category_stats(self.daily_stats['categories'])}
            """)
            
            return daily_package
            
        except Exception as e:
            logger.error(f"âŒ GÃ¼nlÃ¼k iÃ§erik Ã¼retim hatasÄ±: {e}")
            return None
    
    def _optimize_posting_schedule(self, posts: List[Dict]) -> List[Dict]:
        """PostlarÄ± peak hours'a optimize ederek daÄŸÄ±t"""
        scheduled = []
        
        # Posts'u engagement score'a gÃ¶re sÄ±rala
        sorted_posts = sorted(posts, 
                            key=lambda x: x.get('engagement_prediction', {}).get('score', 0), 
                            reverse=True)
        
        # Peak hours'a en iyi postlarÄ± yerleÅŸtir
        for i, post in enumerate(sorted_posts):
            if i < len(self.peak_hours):
                hour = self.peak_hours[i]
                post_time = datetime.now().replace(hour=hour, minute=0, second=0)
            else:
                # Remaining posts'u off-peak hours'a daÄŸÄ±t
                off_peak = [h for h in range(8, 24) if h not in self.peak_hours]
                hour = off_peak[i % len(off_peak)]
                post_time = datetime.now().replace(hour=hour, minute=30, second=0)
            
            scheduled_post = {
                **post,
                "scheduled_time": post_time.isoformat(),
                "priority": "peak" if i < len(self.peak_hours) else "normal",
                "slot_index": i
            }
            
            scheduled.append(scheduled_post)
        
        return scheduled
    
    def _analyze_categories(self, posts: List[Dict]) -> Dict:
        """Post kategori analizi"""
        categories = {}
        for post in posts:
            cat = post.get('category', 'unknown')
            if cat not in categories:
                categories[cat] = 0
            categories[cat] += 1
        return categories
    
    def _calculate_total_engagement(self, posts: List[Dict]) -> float:
        """Toplam engagement tahmini"""
        total_score = sum(post.get('engagement_prediction', {}).get('score', 0) for post in posts)
        return total_score / len(posts) if posts else 0
    
    def _format_category_stats(self, categories: Dict) -> str:
        """Kategori istatistiklerini formatla"""
        lines = []
        for cat, count in categories.items():
            lines.append(f"   â€¢ {cat.title()}: {count} post")
        return "\n".join(lines)
    
    async def publish_scheduled_posts(self):
        """ZamanlanmÄ±ÅŸ postlarÄ± yayÄ±nla"""
        try:
            # BugÃ¼nÃ¼n dosyasÄ±nÄ± oku
            today = datetime.now().strftime("%Y%m%d")
            filename = f"{self.generated_content_dir}/daily_{today}.json"
            
            if not os.path.exists(filename):
                logger.warning(f"âš ï¸ BugÃ¼nÃ¼n iÃ§erik dosyasÄ± bulunamadÄ±: {filename}")
                return
            
            with open(filename, 'r', encoding='utf-8') as f:
                daily_package = json.load(f)
            
            current_time = datetime.now()
            published_count = 0
            
            for post in daily_package.get('scheduled_posts', []):
                scheduled_time = datetime.fromisoformat(post['scheduled_time'])
                
                # ZamanÄ± gelmiÅŸ postlarÄ± yayÄ±nla
                if scheduled_time <= current_time:
                    success = await self._publish_post(post)
                    if success:
                        published_count += 1
                        post['published'] = True
                        post['published_at'] = current_time.isoformat()
            
            # GÃ¼ncellenmis package'Ä± kaydet
            if published_count > 0:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(daily_package, f, ensure_ascii=False, indent=2)
                
                self.daily_stats["posts_published"] += published_count
                logger.info(f"ğŸ“¤ {published_count} post yayÄ±nlandÄ±")
            
        except Exception as e:
            logger.error(f"âŒ Post yayÄ±nlama hatasÄ±: {e}")
    
    async def _publish_post(self, post: Dict) -> bool:
        """Tek post yayÄ±nla (implement platform-specific logic)"""
        try:
            # Telegram channel'a gÃ¶nder
            success_telegram = await self._post_to_telegram(post)
            
            # Instagram'a gÃ¶nder (future implementation)
            # success_instagram = await self._post_to_instagram(post)
            
            # Analytics kaydet
            await self._log_post_analytics(post, success_telegram)
            
            return success_telegram
            
        except Exception as e:
            logger.error(f"âŒ Post publish error: {e}")
            return False
    
    async def _post_to_telegram(self, post: Dict) -> bool:
        """Telegram channel'a post gÃ¶nder"""
        try:
            # Bu kÄ±sÄ±m telegram bot entegrasyonu ile implement edilecek
            # Åimdilik simulation
            
            content = post['content']
            hashtags = " ".join(post.get('hashtags', []))
            full_message = f"{content}\n\n{hashtags}"
            
            logger.info(f"ğŸ“¤ Telegram post simulated: {post['id']}")
            logger.debug(f"Content preview: {content[:100]}...")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Telegram post error: {e}")
            return False
    
    async def _log_post_analytics(self, post: Dict, success: bool):
        """Post analytics kaydet"""
        try:
            analytics_data = {
                "post_id": post['id'],
                "published_at": datetime.now().isoformat(),
                "success": success,
                "category": post.get('category'),
                "engagement_prediction": post.get('engagement_prediction'),
                "word_count": post.get('metadata', {}).get('word_count', 0),
                "hashtags_count": len(post.get('hashtags', [])),
                "platform": "telegram"
            }
            
            # Daily analytics file
            date_str = datetime.now().strftime("%Y%m%d")
            analytics_file = f"{self.analytics_dir}/daily_analytics_{date_str}.json"
            
            # Load existing analytics
            if os.path.exists(analytics_file):
                with open(analytics_file, 'r', encoding='utf-8') as f:
                    daily_analytics = json.load(f)
            else:
                daily_analytics = {"posts": []}
            
            # Add new post
            daily_analytics["posts"].append(analytics_data)
            
            # Save updated analytics
            with open(analytics_file, 'w', encoding='utf-8') as f:
                json.dump(daily_analytics, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"âŒ Analytics logging error: {e}")
    
    def _scheduled_post_generation(self):
        """Scheduled job for post generation"""
        asyncio.run(self.publish_scheduled_posts())
    
    def _daily_analytics_report(self):
        """Daily analytics raporu"""
        try:
            today = datetime.now().strftime("%Y%m%d")
            analytics_file = f"{self.analytics_dir}/daily_analytics_{today}.json"
            
            if os.path.exists(analytics_file):
                with open(analytics_file, 'r', encoding='utf-8') as f:
                    analytics = json.load(f)
                
                total_posts = len(analytics.get('posts', []))
                successful_posts = sum(1 for p in analytics['posts'] if p.get('success'))
                
                logger.info(f"""
ğŸ“Š GÃœNLÃœK ANALYTÄ°CS RAPORU ({today})

âœ… BaÅŸarÄ±lÄ±: {successful_posts}/{total_posts}
ğŸ“ˆ BaÅŸarÄ± oranÄ±: {(successful_posts/total_posts)*100:.1f}%
                """)
                
        except Exception as e:
            logger.error(f"âŒ Analytics report error: {e}")
    
    def _weekly_content_batch(self):
        """Weekly batch content generation"""
        asyncio.run(self._generate_weekly_batch())
    
    async def _generate_weekly_batch(self):
        """HaftalÄ±k batch content Ã¼ret"""
        try:
            logger.info("ğŸ“… HaftalÄ±k batch content Ã¼retimi baÅŸlÄ±yor...")
            
            # 7 gÃ¼nlÃ¼k content package
            weekly_posts = []
            for day in range(7):
                daily_posts = await ai_content_generator.generate_batch_content(
                    count=self.daily_posts_target,
                    distribution=self.content_distribution
                )
                
                if daily_posts:
                    weekly_posts.extend(daily_posts)
                
                # Rate limiting
                await asyncio.sleep(2)
            
            # Save weekly package
            week_start = datetime.now()
            filename = f"{self.generated_content_dir}/weekly_{week_start.strftime('%Y%W')}.json"
            
            weekly_package = {
                "week_start": week_start.isoformat(),
                "total_posts": len(weekly_posts),
                "daily_target": self.daily_posts_target,
                "posts": weekly_posts
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(weekly_package, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… HaftalÄ±k batch hazÄ±r: {len(weekly_posts)} post - {filename}")
            
        except Exception as e:
            logger.error(f"âŒ Weekly batch error: {e}")
    
    async def run_pipeline(self):
        """Ana pipeline loop"""
        try:
            await self.initialize()
            
            logger.info("ğŸš€ SeferVerse AI Pipeline Ã§alÄ±ÅŸÄ±yor...")
            
            # Ä°lk gÃ¼nlÃ¼k content Ã¼ret
            await self.generate_daily_content()
            
            # Scheduling loop
            while self.is_running:
                schedule.run_pending()
                await asyncio.sleep(60)  # Check every minute
                
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Pipeline durduruldu")
        except Exception as e:
            logger.error(f"âŒ Pipeline error: {e}")
        finally:
            self.is_running = False


# Test & Demo functions
async def test_ai_pipeline():
    """AI Pipeline test fonksiyonu"""
    pipeline = SeferVerseAIPipeline()
    
    # Single post test
    print("ğŸ§ª Test: Single post generation")
    single_post = await ai_content_generator.generate_seferverse_post(
        category="motivation",
        theme="baÅŸarÄ±"
    )
    
    if single_post:
        print(f"âœ… Test post generated: {single_post['id']}")
        print(f"ğŸ“ Content preview: {single_post['content'][:100]}...")
        print(f"ğŸ“Š Engagement: {single_post['engagement_prediction']['score']}")
    
    # Daily content test
    print("\nğŸ§ª Test: Daily content package")
    daily_package = await pipeline.generate_daily_content()
    
    if daily_package:
        print(f"âœ… Daily package: {daily_package['total_posts']} posts")
        print(f"ğŸ“ˆ Avg engagement: {daily_package['estimated_engagement']:.1f}")


async def demo_ai_generation():
    """AI Generation demo"""
    print("""
ğŸ¤– SEFERVERSE AI DEMO
====================
GPT-4 ile otomatik iÃ§erik Ã¼retimi
""")
    
    # AI generator'Ä± baÅŸlat
    await ai_content_generator.initialize()
    
    categories = ["motivation", "tech", "lifestyle", "business"]
    
    for category in categories:
        print(f"\nğŸ¯ Kategori: {category.upper()}")
        post = await ai_content_generator.generate_seferverse_post(category=category)
        
        if post:
            print(f"ğŸ“ Ä°Ã§erik: {post['content'][:150]}...")
            print(f"ğŸ·ï¸ Hashtags: {', '.join(post['hashtags'][:5])}")
            print(f"ğŸ“Š Engagement: {post['engagement_prediction']['score']}/100")
        
        await asyncio.sleep(1)
    
    print("\nâœ… Demo tamamlandÄ±!")


# Singleton instance
seferverse_ai_pipeline = SeferVerseAIPipeline()


if __name__ == "__main__":
    # Demo Ã§alÄ±ÅŸtÄ±r
    asyncio.run(demo_ai_generation()) 