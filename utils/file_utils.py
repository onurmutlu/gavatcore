#!/usr/bin/env python3
# utils/file_utils.py
"""
Dosya iÅŸlemleri yardÄ±mcÄ± modÃ¼lÃ¼.
JSON dosyalarÄ±nÄ± gÃ¼venli ÅŸekilde okuma/yazma, kilitleme ve yedekleme iÅŸlemleri.

Production-ready Ã¶zellikler:
- Cross-platform dosya kilitleme
- BÃ¼yÃ¼k dosya desteÄŸi (streaming)
- Atomik yazma ve yedekleme
- Otomatik kurtarma (recovery)
- Åema doÄŸrulama
- Transactional multi-write
- Cluster-safe locking
"""

import os
import json
import shutil
import asyncio
import logging
import tempfile
import zstandard  # type: ignore
from typing import Dict, Any, Optional, Union, BinaryIO, List, Tuple, Generator, Iterator, cast
from pathlib import Path
from filelock import FileLock
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager

# Tip gÃ¼venliÄŸi iÃ§in
import ijson  # type: ignore
import portalocker  # type: ignore
import jsonschema
import redis
import redis_lock  # type: ignore
import threading
import multiprocessing
import time
from datetime import datetime

# YapÄ±landÄ±rma dosyasÄ±/env'den ayarlarÄ± oku
from config import (
    FILE_BACKUP_DIR,
    MAX_BACKUP_COUNT,
    DEFAULT_ENCODING,
    LOG_LEVEL,
    REDIS_HOST,
    REDIS_PORT,
    REDIS_PASSWORD
)

# YapÄ±landÄ±rÄ±lmÄ±ÅŸ loglama
import structlog
logger = structlog.get_logger("gavatcore.file_utils")
logger = logger.bind(
    pid=os.getpid(),
    thread_id=threading.get_ident()
)

# Sabitler
BACKUP_ARCHIVE_FORMAT = "zst"  # zstandard sÄ±kÄ±ÅŸtÄ±rma
LARGE_FILE_THRESHOLD = 10 * 1024 * 1024  # 10 MB
MAX_MEMORY_BUFFER = 1 * 1024 * 1024  # 1 MB chunk size
LOCK_TIMEOUT = 30  # saniye
IO_THREAD_POOL = ThreadPoolExecutor(max_workers=4)

# Redis baÄŸlantÄ±sÄ± (distributed locking iÃ§in)
try:
    redis_client: Optional[redis.Redis[str]] = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        password=REDIS_PASSWORD,
        decode_responses=True
    )
except:
    logger.warning("[REDIS] BaÄŸlantÄ± kurulamadÄ±, yerel kilitleme kullanÄ±lacak")
    redis_client = None

class FileOperationError(Exception):
    """Dosya iÅŸlemleri iÃ§in Ã¶zel hata sÄ±nÄ±fÄ±."""
    pass

class TransactionError(Exception):
    """Transaction iÅŸlemleri iÃ§in Ã¶zel hata sÄ±nÄ±fÄ±."""
    pass

@contextmanager
def distributed_lock(resource_key: str, timeout: int = LOCK_TIMEOUT):
    """
    Distributed (Redis tabanlÄ±) veya yerel dosya kilidi oluÅŸturur.
    
    Args:
        resource_key: Kilitlenecek kaynak anahtarÄ±
        timeout: Kilit zaman aÅŸÄ±mÄ± (saniye)
    """
    lock = None
    try:
        if redis_client and redis_client.ping():
            # Redis tabanlÄ± distributed lock
            lock = redis_lock.Lock(redis_client, resource_key, expire=timeout)
            acquired = lock.acquire(timeout=timeout)
            if not acquired:
                raise FileOperationError(f"Kilit alÄ±namadÄ±: {resource_key}")
        else:
            # Yerel dosya kilidi
            lock = portalocker.Lock(
                resource_key + ".lock",
                timeout=timeout,
                flags=portalocker.LOCK_EX | portalocker.LOCK_NB
            )
            lock.acquire()
        
        logger.debug("[LOCK] Kilit alÄ±ndÄ±", 
                    resource=resource_key, 
                    lock_type="redis" if redis_client else "local")
        yield
        
    finally:
        if lock:
            try:
                if isinstance(lock, redis_lock.Lock):
                    lock.release()
                else:
                    lock.release()
                logger.debug("[LOCK] Kilit bÄ±rakÄ±ldÄ±", 
                           resource=resource_key,
                           lock_type="redis" if redis_client else "local")
            except:
                logger.warning("[LOCK] Kilit bÄ±rakÄ±lamadÄ±", 
                             resource=resource_key,
                             exc_info=True)

def transactional_save_json(
    files_and_data: Dict[str, Dict],
    backup: bool = True,
    pretty: bool = True,
    encoding: str = DEFAULT_ENCODING,
    schema: Optional[Dict] = None
) -> bool:
    """
    Birden fazla JSON dosyasÄ±nÄ± transaction olarak kaydeder.
    Ya hepsi baÅŸarÄ±lÄ± olur ya da hiÃ§biri deÄŸiÅŸmez.
    
    Args:
        files_and_data: Dosya yolu -> JSON verisi eÅŸleÅŸmeleri
        backup: Mevcut dosyalarÄ± yedekle
        pretty: JSON'u gÃ¼zel biÃ§imlendir
        encoding: Dosya kodlamasÄ±
        schema: JSON ÅŸema doÄŸrulamasÄ± iÃ§in ÅŸema (opsiyonel)
        
    Returns:
        Ä°ÅŸlem baÅŸarÄ±lÄ± ise True
    """
    temp_files = {}  # Orijinal dosya -> GeÃ§ici dosya eÅŸleÅŸmeleri
    backups = {}     # Orijinal dosya -> Yedek dosya eÅŸleÅŸmeleri
    
    try:
        # 1. Åema doÄŸrulamasÄ± (varsa)
        if schema:
            for file_path, data in files_and_data.items():
                try:
                    jsonschema.validate(instance=data, schema=schema)
                except jsonschema.exceptions.ValidationError as e:
                    logger.error("[TRANSACTION] Åema doÄŸrulama hatasÄ±", 
                               file=file_path, error=str(e))
                    return False
        
        # 2. TÃ¼m dosyalar iÃ§in geÃ§ici dosya oluÅŸtur
        for file_path, data in files_and_data.items():
            temp_fd, temp_path = tempfile.mkstemp(suffix='.json')
            os.close(temp_fd)
            
            with open(temp_path, 'w', encoding=encoding) as f:
                if pretty:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                else:
                    json.dump(data, f, ensure_ascii=False)
            
            temp_files[file_path] = temp_path
            logger.debug("[TRANSACTION] GeÃ§ici dosya oluÅŸturuldu", 
                        original=file_path, temp=temp_path)
        
        # 3. Yedekleme (istenirse)
        if backup:
            for file_path in files_and_data.keys():
                if os.path.exists(file_path):
                    backup_path = backup_file(file_path)
                    if backup_path:
                        backups[file_path] = backup_path
        
        # 4. TÃ¼m dosyalarÄ± atomik olarak gÃ¼ncelle
        for file_path, temp_path in temp_files.items():
            ensure_directory(file_path)
            with distributed_lock(file_path):
                shutil.move(temp_path, file_path)
                logger.info("[TRANSACTION] Dosya gÃ¼ncellendi", file=file_path)
        
        logger.info("[TRANSACTION] BaÅŸarÄ±lÄ±", 
                   files=list(files_and_data.keys()))
        return True
        
    except Exception as e:
        logger.error("[TRANSACTION] Hata", error=str(e), exc_info=True)
        
        # Rollback: GeÃ§ici dosyalarÄ± temizle
        for temp_path in temp_files.values():
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        # Yedeklerden geri yÃ¼kle
        if backup and backups:
            for file_path, backup_path in backups.items():
                try:
                    shutil.copy2(backup_path, file_path)
                    logger.info("[TRANSACTION] Yedekten geri yÃ¼klendi", 
                              file=file_path, backup=backup_path)
                except Exception as restore_error:
                    logger.error("[TRANSACTION] Geri yÃ¼kleme hatasÄ±", 
                               file=file_path, error=str(restore_error))
        
        return False

def get_file_lock(file_path: str) -> portalocker.Lock:
    """
    Cross-platform dosya kilidi oluÅŸturur.
    
    Args:
        file_path: Kilitlenecek dosya yolu
        
    Returns:
        Dosya kilidi nesnesi
    """
    return portalocker.Lock(
        file_path + ".lock",
        timeout=LOCK_TIMEOUT,
        flags=portalocker.LOCK_EX | portalocker.LOCK_NB
    )

def ensure_directory(file_path: str) -> None:
    """
    Dosya yolundaki dizinlerin var olduÄŸundan emin olur.
    
    Args:
        file_path: Dosya yolu
    """
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        logger.debug(f"[DIRECTORY] OluÅŸturuldu: {directory}")

def is_large_file(file_path: str) -> bool:
    """
    DosyanÄ±n bÃ¼yÃ¼k olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    
    Args:
        file_path: Kontrol edilecek dosya yolu
        
    Returns:
        Dosya bÃ¼yÃ¼k mÃ¼
    """
    try:
        return os.path.getsize(file_path) > LARGE_FILE_THRESHOLD
    except OSError:
        return False

def compress_backup(backup_path: str) -> str:
    """
    Yedek dosyayÄ± sÄ±kÄ±ÅŸtÄ±rÄ±r.
    
    Args:
        backup_path: SÄ±kÄ±ÅŸtÄ±rÄ±lacak dosya yolu
        
    Returns:
        SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ dosya yolu
    """
    compressed_path = f"{backup_path}.{BACKUP_ARCHIVE_FORMAT}"
    try:
        with open(backup_path, 'rb') as f_in:
            with open(compressed_path, 'wb') as f_out:
                compressor = zstandard.ZstdCompressor()
                compressor.copy_stream(f_in, f_out)
        os.unlink(backup_path)  # Orijinal dosyayÄ± sil
        logger.info(f"[COMPRESS] Yedek sÄ±kÄ±ÅŸtÄ±rÄ±ldÄ±: {compressed_path}")
        return compressed_path
    except Exception as e:
        logger.error(f"[COMPRESS] SÄ±kÄ±ÅŸtÄ±rma hatasÄ±: {e}")
        if os.path.exists(compressed_path):
            os.unlink(compressed_path)
        return backup_path

def decompress_backup(compressed_path: str) -> str:
    """
    SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ yedek dosyayÄ± aÃ§ar.
    
    Args:
        compressed_path: SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ dosya yolu
        
    Returns:
        AÃ§Ä±lmÄ±ÅŸ dosya yolu
    """
    decompressed_path = compressed_path.rsplit(f".{BACKUP_ARCHIVE_FORMAT}", 1)[0]
    try:
        with open(compressed_path, 'rb') as f_in:
            with open(decompressed_path, 'wb') as f_out:
                decompressor = zstandard.ZstdDecompressor()
                decompressor.copy_stream(f_in, f_out)
        logger.info(f"[DECOMPRESS] Yedek aÃ§Ä±ldÄ±: {decompressed_path}")
        return decompressed_path
    except Exception as e:
        logger.error(f"[DECOMPRESS] AÃ§ma hatasÄ±: {e}")
        if os.path.exists(decompressed_path):
            os.unlink(decompressed_path)
        raise FileOperationError(f"Yedek aÃ§Ä±lamadÄ±: {e}")

def load_json(
    file_path: str, 
    default: Optional[Dict[str, Any]] = None,
    create_if_missing: bool = True,
    encoding: str = DEFAULT_ENCODING,
    schema: Optional[Dict[str, Any]] = None,
    stream: bool = False
) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
    """
    JSON dosyasÄ±nÄ± gÃ¼venli bir ÅŸekilde yÃ¼kler.
    
    Args:
        file_path: YÃ¼klenecek JSON dosyasÄ±nÄ±n yolu
        default: Dosya bulunamazsa veya geÃ§ersizse kullanÄ±lacak varsayÄ±lan deÄŸer
        create_if_missing: Dosya yoksa oluÅŸturulsun mu
        encoding: Dosya kodlamasÄ±
        schema: JSON ÅŸema doÄŸrulamasÄ± iÃ§in ÅŸema (opsiyonel)
        stream: BÃ¼yÃ¼k dosyalar iÃ§in streaming mod kullanÄ±lsÄ±n mÄ±
        
    Returns:
        JSON dosyasÄ±ndan yÃ¼klenen veri sÃ¶zlÃ¼ÄŸÃ¼ veya liste
    """
    if default is None:
        default = {}
    
    try:
        # Dosya var mÄ± kontrol et
        if not os.path.exists(file_path):
            if create_if_missing:
                ensure_directory(file_path)
                save_json(file_path, default, encoding=encoding)
                logger.info("[CREATE] Dosya oluÅŸturuldu", file=file_path)
            return default
        
        # BÃ¼yÃ¼k dosya kontrolÃ¼
        use_streaming = stream or is_large_file(file_path)
        
        # DosyayÄ± aÃ§ ve yÃ¼kle
        with distributed_lock(file_path):
            if use_streaming:
                logger.debug("[STREAM] Streaming modunda okunuyor", file=file_path)
                # DosyayÄ± aÃ§Ä±k tut ve iterator dÃ¶ndÃ¼r
                with open(file_path, 'rb') as f:
                    try:
                        # items array'i iÃ§in
                        items: List[Dict[str, Any]] = []
                        parser = ijson.parse(f)
                        current_item: Optional[Dict[str, Any]] = None
                        
                        for prefix, event, value in parser:
                            if prefix == 'items' and event == 'start_array':
                                continue
                            elif prefix.startswith('items.item'):
                                if event == 'start_map':
                                    current_item = {}
                                elif event == 'end_map':
                                    if current_item is not None:
                                        items.append(current_item)
                                    current_item = None
                                elif current_item is not None:
                                    key = prefix.split('.')[-1]
                                    current_item[key] = value
                        
                        return items
                    except Exception as e:
                        logger.error("[STREAM] Okuma hatasÄ±", 
                                   file=file_path, error=str(e))
                        return cast(Dict[str, Any], default)
            else:
                with open(file_path, 'r', encoding=encoding) as f:
                    try:
                        data = json.load(f)
                        if schema:
                            jsonschema.validate(instance=data, schema=schema)
                        return cast(Dict[str, Any], data)
                    except json.JSONDecodeError as e:
                        logger.error("[LOAD] JSON ayrÄ±ÅŸtÄ±rma hatasÄ±", 
                                   file=file_path, error=str(e))
                        # Otomatik kurtarma dene
                        recovered_data = recover_from_backup(file_path)
                        if recovered_data is not None:
                            return recovered_data
                        return cast(Dict[str, Any], default)
                    except jsonschema.exceptions.ValidationError as e:
                        logger.error("[VALIDATE] Åema doÄŸrulama hatasÄ±", 
                                   file=file_path, error=str(e))
                        return cast(Dict[str, Any], default)
    except Exception as e:
        logger.error("[LOAD] Hata", file=file_path, error=str(e))
        return cast(Dict[str, Any], default)

def save_json(
    file_path: str, 
    data: Dict,
    backup: bool = True,
    pretty: bool = True,
    encoding: str = DEFAULT_ENCODING,
    schema: Optional[Dict] = None,
    compress_backup: bool = True
) -> bool:
    """
    Veriyi JSON dosyasÄ±na gÃ¼venli bir ÅŸekilde kaydeder.
    
    Args:
        file_path: Kaydedilecek JSON dosyasÄ±nÄ±n yolu
        data: Kaydedilecek veri
        backup: Mevcut dosyayÄ± yedekle
        pretty: JSON'u gÃ¼zel biÃ§imlendir
        encoding: Dosya kodlamasÄ±
        schema: JSON ÅŸema doÄŸrulamasÄ± iÃ§in ÅŸema (opsiyonel)
        compress_backup: Yedekleri sÄ±kÄ±ÅŸtÄ±r
        
    Returns:
        BaÅŸarÄ±lÄ± ise True, deÄŸilse False
    """
    try:
        # Åema doÄŸrulamasÄ±
        if schema:
            try:
                jsonschema.validate(instance=data, schema=schema)
            except jsonschema.exceptions.ValidationError as e:
                logger.error(f"[VALIDATE] Veri ÅŸema doÄŸrulamasÄ± baÅŸarÄ±sÄ±z: {e}")
                return False
        
        # Dizini oluÅŸtur
        ensure_directory(file_path)
        
        # GeÃ§ici dosyaya yaz
        with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding=encoding) as temp_file:
            if pretty:
                json.dump(data, temp_file, indent=2, ensure_ascii=False)
            else:
                json.dump(data, temp_file, ensure_ascii=False)
        
        with get_file_lock(file_path):
            # Ã–nceki dosya varsa yedekle
            if backup and os.path.exists(file_path):
                backup_file(file_path, compress=compress_backup)
            
            # GeÃ§ici dosyayÄ± asÄ±l konuma taÅŸÄ± (atomik operasyon)
            shutil.move(temp_file.name, file_path)
        
        logger.debug(f"[SAVE] '{file_path}' baÅŸarÄ±yla kaydedildi")
        return True
    except Exception as e:
        # GeÃ§ici dosyayÄ± temizle
        if 'temp_file' in locals() and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
        
        logger.error(f"[SAVE] '{file_path}' kaydedilirken hata: {e}")
        return False

def backup_file(
    file_path: str, 
    backup_dir: Optional[str] = None, 
    max_backups: int = MAX_BACKUP_COUNT,
    compress: bool = True
) -> Optional[str]:
    """
    DosyanÄ±n yedeÄŸini alÄ±r.
    
    Args:
        file_path: Yedeklenecek dosya yolu
        backup_dir: Yedeklerin kaydedileceÄŸi dizin
        max_backups: Saklanacak maksimum yedek sayÄ±sÄ±
        compress: Yedekleri sÄ±kÄ±ÅŸtÄ±r
        
    Returns:
        Yedek dosyanÄ±n yolu veya hata durumunda None
    """
    try:
        if not os.path.exists(file_path):
            return None
        
        # Yedek dizinini belirle
        if backup_dir is None:
            backup_dir = os.path.join(os.path.dirname(file_path), FILE_BACKUP_DIR)
        
        # Yedek dizinini oluÅŸtur
        os.makedirs(backup_dir, exist_ok=True)
        
        # Dosya adÄ±nÄ± ve uzantÄ±sÄ±nÄ± ayÄ±r
        base_name = os.path.basename(file_path)
        file_name, file_ext = os.path.splitext(base_name)
        
        # Timestamp ile yedek dosya adÄ±nÄ± oluÅŸtur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{file_name}_{timestamp}{file_ext}"
        backup_path = os.path.join(backup_dir, backup_name)
        
        # DosyayÄ± kopyala
        shutil.copy2(file_path, backup_path)
        
        # SÄ±kÄ±ÅŸtÄ±rma
        if compress:
            backup_path = compress_backup(backup_path)
        
        # Eski yedekleri temizle
        clean_old_backups(backup_dir, file_name, file_ext, max_backups)
        
        logger.info(f"[BACKUP] '{file_path}' yedeklendi: '{backup_path}'")
        return backup_path
    except Exception as e:
        logger.error(f"[BACKUP] '{file_path}' yedeklenirken hata: {e}")
        return None

def clean_old_backups(
    backup_dir: str, 
    file_name: str, 
    file_ext: str, 
    max_count: int
) -> None:
    """
    Belirtilen sayÄ±dan fazla yedek varsa en eskileri siler.
    
    Args:
        backup_dir: Yedek dizini
        file_name: Dosya adÄ±
        file_ext: Dosya uzantÄ±sÄ±
        max_count: Saklanacak maksimum yedek sayÄ±sÄ±
    """
    try:
        # Ä°lgili yedek dosyalarÄ±nÄ± bul
        pattern = f"{file_name}_*{file_ext}"
        backup_files = []
        
        # Normal ve sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ yedekleri bul
        backup_files.extend(list(Path(backup_dir).glob(pattern)))
        backup_files.extend(list(Path(backup_dir).glob(f"{pattern}.{BACKUP_ARCHIVE_FORMAT}")))
        
        # Tarihe gÃ¶re sÄ±rala (eskiden yeniye)
        backup_files.sort(key=lambda x: os.path.getmtime(x))
        
        # Eski yedekleri sil
        if len(backup_files) > max_count:
            for old_file in backup_files[:-max_count]:
                try:
                    os.remove(old_file)
                    logger.debug(f"[CLEAN] Eski yedek silindi: '{old_file}'")
                except Exception as e:
                    logger.warning(f"[CLEAN] Yedek silinirken hata: {e}")
    except Exception as e:
        logger.error(f"[CLEAN] Eski yedekler temizlenirken hata: {e}")

def recover_from_backup(file_path: str) -> Optional[Dict]:
    """
    Bozuk dosyayÄ± en yeni yedekten geri yÃ¼kler.
    
    Args:
        file_path: KurtarÄ±lacak dosya yolu
        
    Returns:
        KurtarÄ±lan veri veya None
    """
    try:
        backup_dir = os.path.join(os.path.dirname(file_path), FILE_BACKUP_DIR)
        if not os.path.exists(backup_dir):
            logger.error("[RECOVERY] Yedek dizini bulunamadÄ±", 
                        file=file_path, backup_dir=backup_dir)
            return None
        
        # En yeni yedeÄŸi bul
        file_name = os.path.splitext(os.path.basename(file_path))[0]
        backup_files = []
        
        # Normal ve sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ yedekleri kontrol et
        backup_files.extend(list(Path(backup_dir).glob(f"{file_name}_*.json")))
        backup_files.extend(list(Path(backup_dir).glob(f"{file_name}_*.json.{BACKUP_ARCHIVE_FORMAT}")))
        
        if not backup_files:
            logger.error("[RECOVERY] Yedek bulunamadÄ±", file=file_path)
            return None
        
        # En yeni yedeÄŸi bul
        latest_backup = max(backup_files, key=os.path.getmtime)
        
        # SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ mÄ± kontrol et
        if latest_backup.suffix == f".{BACKUP_ARCHIVE_FORMAT}":
            latest_backup = Path(decompress_backup(str(latest_backup)))
        
        # YedeÄŸi yÃ¼kle ve doÄŸrula
        with open(latest_backup, 'r', encoding=DEFAULT_ENCODING) as f:
            data = json.load(f)
        
        # BaÅŸarÄ±lÄ± recovery - dosyayÄ± gÃ¼ncelle
        with distributed_lock(file_path):
            with open(file_path, 'w', encoding=DEFAULT_ENCODING) as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info("[RECOVERY] BaÅŸarÄ±lÄ±", 
                   file=file_path, 
                   backup=str(latest_backup))
        return data
        
    except Exception as e:
        logger.error("[RECOVERY] Hata", 
                    file=file_path, 
                    error=str(e), 
                    exc_info=True)
        return None

async def load_json_async(
    file_path: str, 
    default: Optional[Dict[str, Any]] = None,
    create_if_missing: bool = True,
    encoding: str = DEFAULT_ENCODING,
    schema: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    JSON dosyasÄ±nÄ± asenkron olarak yÃ¼kler.
    
    Args:
        file_path: YÃ¼klenecek JSON dosyasÄ±nÄ±n yolu
        default: Dosya bulunamazsa veya geÃ§ersizse kullanÄ±lacak varsayÄ±lan deÄŸer
        create_if_missing: Dosya yoksa oluÅŸturulsun mu
        encoding: Dosya kodlamasÄ±
        schema: JSON ÅŸema doÄŸrulamasÄ± iÃ§in ÅŸema (opsiyonel)
        
    Returns:
        JSON dosyasÄ±ndan yÃ¼klenen veri sÃ¶zlÃ¼ÄŸÃ¼
    """
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        IO_THREAD_POOL,
        lambda: load_json(
            file_path, 
            default, 
            create_if_missing, 
            encoding,
            schema,
            stream=False
        )
    )
    return cast(Dict[str, Any], result)

async def save_json_async(
    file_path: str, 
    data: Dict,
    backup: bool = True,
    pretty: bool = True,
    encoding: str = DEFAULT_ENCODING,
    schema: Optional[Dict] = None
) -> bool:
    """
    Veriyi JSON dosyasÄ±na asenkron olarak kaydeder.
    
    Args:
        file_path: Kaydedilecek JSON dosyasÄ±nÄ±n yolu
        data: Kaydedilecek veri
        backup: Mevcut dosyayÄ± yedekle
        pretty: JSON'u gÃ¼zel biÃ§imlendir
        encoding: Dosya kodlamasÄ±
        schema: JSON ÅŸema doÄŸrulamasÄ± iÃ§in ÅŸema (opsiyonel)
        
    Returns:
        BaÅŸarÄ±lÄ± ise True, deÄŸilse False
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        IO_THREAD_POOL,
        lambda: save_json(
            file_path, 
            data, 
            backup, 
            pretty, 
            encoding,
            schema
        )
    )

def merge_json_files(
    target_file: str, 
    source_files: List[str], 
    overwrite: bool = False,
    schema: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Birden fazla JSON dosyasÄ±nÄ± birleÅŸtirir.
    
    Args:
        target_file: Hedef JSON dosyasÄ±
        source_files: Kaynak JSON dosyalarÄ± listesi
        overwrite: Ã‡akÄ±ÅŸma durumunda Ã¼zerine yazÄ±lsÄ±n mÄ±
        schema: BirleÅŸtirme sonrasÄ± doÄŸrulama iÃ§in ÅŸema
        
    Returns:
        BaÅŸarÄ±lÄ± ise True, deÄŸilse False
    """
    try:
        # Hedef dosyayÄ± yÃ¼kle (yoksa boÅŸ dict)
        target_data = cast(Dict[str, Any], load_json(target_file, default={}, create_if_missing=False))
        
        # Kaynak dosyalarÄ± sÄ±rayla birleÅŸtir
        for source_file in source_files:
            source_data = cast(Dict[str, Any], load_json(source_file, default={}, create_if_missing=False))
            
            if overwrite:
                # Ãœzerine yaz
                target_data.update(source_data)
            else:
                # Sadece yeni anahtarlarÄ± ekle
                for key, value in source_data.items():
                    if key not in target_data:
                        target_data[key] = value
        
        # Åema doÄŸrulamasÄ±
        if schema:
            try:
                jsonschema.validate(instance=target_data, schema=schema)
            except jsonschema.exceptions.ValidationError as e:
                logger.error(f"[MERGE] BirleÅŸtirme sonucu ÅŸema doÄŸrulamasÄ± baÅŸarÄ±sÄ±z: {e}")
                return False
        
        # Sonucu kaydet
        return save_json(target_file, target_data)
    except Exception as e:
        logger.error(f"[MERGE] JSON dosyalarÄ± birleÅŸtirilirken hata: {e}")
        return False

def validate_json_schema(data: Dict, schema: Dict) -> Tuple[bool, List[str]]:
    """
    JSON verisini ÅŸemaya gÃ¶re doÄŸrular.
    
    Args:
        data: DoÄŸrulanacak JSON verisi
        schema: JSON ÅemasÄ±
        
    Returns:
        (GeÃ§erli mi, Hata mesajlarÄ± listesi)
    """
    try:
        jsonschema.validate(instance=data, schema=schema)
        return True, []
    except jsonschema.exceptions.ValidationError as e:
        return False, [str(e)]
    except Exception as e:
        return False, [f"DoÄŸrulama hatasÄ±: {str(e)}"]

# Test ve Ã¶rnek kullanÄ±m
if __name__ == "__main__":
    # Test iÃ§in logging yapÄ±landÄ±rmasÄ±
    logging.basicConfig(level=logging.DEBUG)
    
    # Worker process fonksiyonu
    def worker_process(file_path):
        with distributed_lock(file_path):
            # Dosyaya yaz
            with open(file_path, 'w') as f:
                data = {"timestamp": datetime.now().isoformat()}
                json.dump(data, f)
            # Biraz bekle
            time.sleep(0.1)
    
    def run_tests():
        """KapsamlÄ± test senaryolarÄ±."""
        
        print("\nğŸ§ª File Utils Test BaÅŸlÄ±yor...\n")
        
        # Test dosyalarÄ±
        test_file = "test_data.json"
        large_test_file = "test_large.json"
        schema_test_file = "test_schema.json"
        
        # Test verisi
        test_data = {"users": [{"id": 1, "name": "Test User"}], "version": 1}
        
        # JSON Åema Ã¶rneÄŸi
        test_schema = {
            "type": "object",
            "properties": {
                "users": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "integer"},
                            "name": {"type": "string"}
                        },
                        "required": ["id", "name"]
                    }
                },
                "version": {"type": "integer"}
            },
            "required": ["users", "version"]
        }
        
        try:
            # 1. Temel JSON iÅŸlemleri
            print("ğŸ“ Temel JSON iÅŸlemleri testi...")
            save_json(test_file, test_data)
            loaded_data = load_json(test_file)
            assert loaded_data == test_data
            print("âœ… BaÅŸarÄ±lÄ±: Temel JSON iÅŸlemleri")
            
            # 2. Åema doÄŸrulama
            print("\nğŸ” Åema doÄŸrulama testi...")
            is_valid, errors = validate_json_schema(test_data, test_schema)
            assert is_valid
            print("âœ… BaÅŸarÄ±lÄ±: Åema doÄŸrulama")
            
            # 3. BÃ¼yÃ¼k dosya testi
            print("\nğŸ“¦ BÃ¼yÃ¼k dosya testi...")
            # BÃ¼yÃ¼k JSON array oluÅŸtur
            large_data = []
            for i in range(1000):
                large_data.append({
                    "id": i,
                    "data": "x" * 100
                })
            save_json(large_test_file, {"items": large_data})
            
            # Streaming okuma
            print("ğŸ”„ Streaming okuma testi...")
            loaded_items = load_json(large_test_file, stream=True)
            assert len(loaded_items) == len(large_data)
            print("âœ… BaÅŸarÄ±lÄ±: BÃ¼yÃ¼k dosya iÅŸlemleri")
            
            # 4. Yedekleme ve kurtarma
            print("\nğŸ’¾ Yedekleme ve kurtarma testi...")
            test_data = {"data": "test"}
            save_json(test_file, test_data)
            backup_path = backup_file(test_file)
            assert backup_path is not None
            
            # DosyayÄ± boz
            with open(test_file, 'w') as f:
                f.write("invalid json{")
            
            # Kurtarma dene
            recovered_data = recover_from_backup(test_file)
            assert recovered_data == test_data
            print("âœ… BaÅŸarÄ±lÄ±: Yedekleme ve kurtarma")
            
            # 5. Asenkron iÅŸlemler
            print("\nâš¡ Asenkron iÅŸlemler testi...")
            async def async_test():
                await save_json_async("async_test.json", test_data)
                loaded = await load_json_async("async_test.json")
                assert loaded == test_data
            
            asyncio.run(async_test())
            print("âœ… BaÅŸarÄ±lÄ±: Asenkron iÅŸlemler")
            
            # 6. Dosya birleÅŸtirme
            print("\nğŸ”„ Dosya birleÅŸtirme testi...")
            data1 = {"a": 1, "b": 2}
            data2 = {"c": 3, "d": 4}
            
            save_json("test1.json", data1)
            save_json("test2.json", data2)
            
            merge_result = merge_json_files(
                "merged.json",
                ["test1.json", "test2.json"]
            )
            assert merge_result
            
            merged_data = load_json("merged.json")
            assert len(merged_data) == 4
            print("âœ… BaÅŸarÄ±lÄ±: Dosya birleÅŸtirme")
            
            # 7. Transactional Multi-Write Testi
            print("\nğŸ’¾ Transactional multi-write testi...")
            files_data = {
                "test1.json": {"data": "file1"},
                "test2.json": {"data": "file2"},
                "test3.json": {"data": "file3"}
            }
            
            success = transactional_save_json(files_data)
            assert success
            
            # DosyalarÄ± kontrol et
            for file_path, expected_data in files_data.items():
                assert os.path.exists(file_path)
                loaded_data = load_json(file_path)
                assert loaded_data == expected_data
            
            print("âœ… BaÅŸarÄ±lÄ±: Transactional multi-write")
            
            # 8. Recovery Testi
            print("\nğŸ”„ Recovery testi...")
            # Ã–nce yedek al
            backup_path = backup_file("test1.json")
            assert backup_path is not None
            
            # DosyayÄ± boz
            with open("test1.json", 'w') as f:
                f.write("invalid{json")
            
            # Recovery dene
            recovered_data = recover_from_backup("test1.json")
            assert recovered_data is not None
            assert recovered_data == {"data": "file1"}
            print("âœ… BaÅŸarÄ±lÄ±: Recovery")
            
            # 9. Ã‡oklu Process Lock Testi
            print("\nğŸ”’ Multi-process lock testi...")
            
            # 5 process baÅŸlat
            processes = []
            for _ in range(5):
                p = multiprocessing.Process(
                    target=worker_process, 
                    args=("shared.json",)
                )
                p.start()
                processes.append(p)
            
            # TÃ¼m process'leri bekle
            for p in processes:
                p.join()
            
            print("âœ… BaÅŸarÄ±lÄ±: Multi-process locking")
            
        except AssertionError:
            print("âŒ Test baÅŸarÄ±sÄ±z!")
            raise
        except Exception as e:
            print(f"âŒ Test hatasÄ±: {e}")
            raise
        finally:
            # Temizlik
            for f in [test_file, large_test_file, schema_test_file, 
                     "async_test.json", "test1.json", "test2.json", 
                     "test3.json", "merged.json", "shared.json"]:
                if os.path.exists(f):
                    os.unlink(f)
            
            # Yedek dizinini temizle
            backup_dir = os.path.join(os.path.dirname(test_file), FILE_BACKUP_DIR)
            if os.path.exists(backup_dir):
                shutil.rmtree(backup_dir)
        
        print("\nğŸ‰ TÃ¼m testler baÅŸarÄ±yla tamamlandÄ±!")
    
    # Testleri Ã§alÄ±ÅŸtÄ±r
    run_tests() 